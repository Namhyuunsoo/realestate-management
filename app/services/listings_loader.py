# listings_loader.py
# app/services/listings_loader.py

import os
import json
import pandas as pd
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
from flask import current_app
from .sheet_fetcher import read_local_listing_sheet
from ..core.ids import listing_id_from_row
from ..core.utils import to_int_or_none
from ..models.listing_schema import Listing

# í—¤ë” ê¸°ëŒ€ (ìµœì†Œ)
EXPECTED_HEADERS = [
    "ì ‘ìˆ˜ë‚ ì§œ","ì§€ì—­","ì§€ë²ˆ","ê±´ë¬¼ëª…","ì¸µìˆ˜","ê°€ê²Œëª…","ë¶„ì–‘","ì‹¤í‰ìˆ˜",
    "ë³´ì¦ê¸ˆ","ì›”ì„¸","ê¶Œë¦¬ê¸ˆ","ë¹„ê³ ","ë‹´ë‹¹ì","í˜„í™©","ì§€ì—­2","ì—°ë½ì²˜",
    "ì˜ë¢°ì¸","ë¹„ê³ 3","ìœ„ë°˜ì—¬ë¶€","í˜„ìˆ˜ë§‰ë²ˆí˜¸"
]

class CacheEntry:
    """TTL ê¸°ë°˜ ìºì‹œ ì—”íŠ¸ë¦¬ í´ë˜ìŠ¤"""
    def __init__(self, data: Any, ttl_seconds: int = 300):
        self.data = data
        self.created_at = datetime.now()
        self.ttl_seconds = ttl_seconds
    
    def is_expired(self) -> bool:
        """ìºì‹œê°€ ë§Œë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        return datetime.now() - self.created_at > timedelta(seconds=self.ttl_seconds)

# TTL ê¸°ë°˜ ìºì‹œ (ê¸°ì¡´ ì „ì—­ ë³€ìˆ˜ ëŒ€ì²´)
_cache: Optional[CacheEntry] = None

def read_map_cache() -> Dict[str, tuple[float,float]]:
    """
    data/raw/ì§€ë„ìºì‹œ.xlsx ì˜ 'ì§€ë„ìºì‹œ' ì‹œíŠ¸ë¥¼ ì½ì–´ì„œ
    {ì£¼ì†Œ: (lat, lng)} ë§¤í•‘ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    fn   = current_app.config["MAP_CACHE_FILENAME"]  # "ì§€ë„ìºì‹œ.xlsx"
    data_dir = current_app.config["DATA_DIR"]
    path = os.path.join(data_dir, "raw", fn)
    if not os.path.exists(path):
        current_app.logger.warning(f"Map cache not found: {path}")
        return {}

    try:
        # Excel íŒŒì¼ ì—´ê³  ì‹œíŠ¸ ì´ë¦„ ëª©ë¡ì„ í™•ì¸
        xls = pd.ExcelFile(path)
        # ë§Œì•½ 'ì§€ë„ìºì‹œ' ì‹œíŠ¸ê°€ ìˆìœ¼ë©´, ê·¸ê±¸ ì“°ê³  ì•„ë‹ˆë©´ ì²« ë²ˆì§¸ ì‹œíŠ¸ ì‚¬ìš©
        sheet = "ì§€ë„ìºì‹œ" if "ì§€ë„ìºì‹œ" in xls.sheet_names else xls.sheet_names[0]
        current_app.logger.info(f"Using sheet for map cache: {sheet}")

        df = pd.read_excel(path, sheet_name=sheet, dtype=str).fillna("")

        mapping: dict[str, tuple[float,float]] = {}
        for _, row in df.iterrows():
            addr = row.get("ì£¼ì†Œ","").strip()
            lat  = row.get("ìœ„ë„","").strip()
            lng  = row.get("ê²½ë„","").strip()
            if addr and lat and lng:
                coords = safe_parse_coordinates(lat, lng)
                if coords:
                    mapping[addr] = coords
                else:
                    current_app.logger.warning(f"Invalid coordinates for {addr}: {lat}/{lng}")
        
        current_app.logger.info(f"Loaded {len(mapping)} coordinate mappings")
        return mapping
        
    except Exception as e:
        current_app.logger.error(f"Failed to read map cache: {e}")
        return {}

def safe_parse_coordinates(lat: str, lng: str) -> Optional[tuple[float, float]]:
    """ì•ˆì „í•˜ê²Œ ì¢Œí‘œë¥¼ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜"""
    try:
        lat_val = float(lat)
        lng_val = float(lng)
        
        # ìœ íš¨í•œ ì¢Œí‘œ ë²”ìœ„ ê²€ì¦
        if -90 <= lat_val <= 90 and -180 <= lng_val <= 180:
            return (lat_val, lng_val)
        else:
            current_app.logger.warning(f"Coordinates out of valid range: {lat_val}, {lng_val}")
            return None
            
    except (ValueError, TypeError) as e:
        current_app.logger.warning(f"Invalid coordinate format: {lat}/{lng} - {e}")
        return None

def normalize_headers(header_row: list[str]) -> Dict[str, int]:
    mapping = {}
    for name in EXPECTED_HEADERS:
        if name in header_row:
            mapping[name] = header_row.index(name)
    return mapping

def build_address(row: list[str], hdr: Dict[str,int]) -> str:
    try:
        region2 = row[hdr["ì§€ì—­2"]].strip()
        region = row[hdr["ì§€ì—­"]].strip()
        lot = row[hdr["ì§€ë²ˆ"]].strip()
        return f"{region2} {region} {lot}".strip()
    except:
        return ""

def normalize_listing(row_idx: int, row: list[str], hdr: Dict[str,int]) -> Listing | None:
    try:
        status_raw = row[hdr["í˜„í™©"]].strip()
        # ë””ë²„ê·¸: í˜„í™© ê°’ í™•ì¸
        if row_idx <= 5:  # ì²˜ìŒ 5ê°œ í–‰ë§Œ ë¡œê·¸
            current_app.logger.info(f"Row {row_idx}: í˜„í™© = '{status_raw}' (ì›ë³¸: '{row[hdr['í˜„í™©']]}')")
    except Exception as e:
        current_app.logger.error(f"Row {row_idx}: í˜„í™© ì½ê¸° ì‹¤íŒ¨ - {e}")
        status_raw = ""

    address_full = build_address(row, hdr)
    if address_full.strip() == "":
        return None

    def gv(col):  # get value
        return row[hdr[col]] if col in hdr else ""

    fields = { col: gv(col) for col in hdr.keys() }

    # numeric parsing (ë§Œì› ê¸°ì¤€ì´ë¼ë©´ ê·¸ëŒ€ë¡œ int)
    deposit = to_int_or_none(fields.get("ë³´ì¦ê¸ˆ"))
    rent = to_int_or_none(fields.get("ì›”ì„¸"))
    premium = to_int_or_none(fields.get("ê¶Œë¦¬ê¸ˆ"))
    area = to_int_or_none(fields.get("ì‹¤í‰ìˆ˜"))
    total = None
    if deposit is not None and premium is not None:
        total = deposit + premium

    listing = Listing(
        id=listing_id_from_row(row_idx),
        raw_row_index=row_idx,
        address_full=address_full,
        address_comp={
            "region2": fields.get("ì§€ì—­2",""),
            "region": fields.get("ì§€ì—­",""),
            "lot": fields.get("ì§€ë²ˆ","")
        },
        fields=fields,
        coords={"lat": None, "lng": None},  # S2 ì´í›„ geocode fill
        numeric_cache={
            "deposit": deposit,
            "rent": rent,
            "premium": premium,
            "area": area,
            "total": total
        },
        status_raw=status_raw
    )
    return listing

def load_listings(force_reload=False) -> List[dict]:
    """
    ë§¤ë¬¼ ë°ì´í„° ë¡œë“œ
    force_reload=True ì‹œ ìºì‹œ ë¬´ì‹œí•˜ê³  íŒŒì¼ì—ì„œ ì§ì ‘ ì½ê¸°
    """
    if force_reload:
        current_app.logger.info("ğŸ”„ ê°•ì œ ìƒˆë¡œê³ ì¹¨: ìºì‹œ ë¬´ì‹œí•˜ê³  íŒŒì¼ì—ì„œ ì§ì ‘ ë¡œë“œ")
    else:
        current_app.logger.info("ğŸ“Š ì¼ë°˜ ë¡œë“œ: ìºì‹œ ìš°ì„  ì‚¬ìš©")
    
    # sheet_fetcherì— force_reload íŒŒë¼ë¯¸í„° ì „ë‹¬
    rows = read_local_listing_sheet(force_reload=force_reload)
    
    header = rows[0]
    hdr_map = normalize_headers(header)
    missing = [h for h in EXPECTED_HEADERS if h not in hdr_map]
    if missing:
        current_app.logger.warning(f"Missing headers: {missing}")

    listings: List[dict] = []
    for i, row in enumerate(rows[1:], start=1):
        listing = normalize_listing(i, row, hdr_map)
        if listing:
            listings.append(listing.to_dict())

    # ì§€ë„ìºì‹œ ë§¤í•‘ ì ìš©
    _apply_map_cache(listings)
    
    current_app.logger.info(f"âœ… Loaded listings: {len(listings)} (force_reload: {force_reload})")
    return listings

def _apply_map_cache(listings: List[dict]) -> None:
    """ì§€ë„ ìºì‹œ ë§¤í•‘ ì ìš©"""
    try:
        map_cache = read_map_cache()
        for item in listings:
            addr = item.get("address_full", "")
            if addr in map_cache and item.get("status_raw") == "ìƒ":
                lat, lng = map_cache[addr]
                item["coords"] = {"lat": lat, "lng": lng}
            else:
                item["coords"] = {"lat": None, "lng": None}
    except Exception as e:
        current_app.logger.warning(f"ì§€ë„ìºì‹œ ë§¤í•‘ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
        for item in listings:
            item["coords"] = {"lat": None, "lng": None}
